{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpusML.txt', 'r') as f:\n",
    "    corpus = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [unidecode(line.lower()) for line in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comence a trabajar y me pegaron, me maltrataron con chicote \\n',\n",
       " 'mis patrones me pegaron porque no me queria apurar, porque era flojo \\n']"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1, 2) Limpiar corpus y agregar simbolos de inicio y fin\n",
    "\n",
    "* Se limpia el corpus mediante el algoritmo de Porter para el lenguaje espa;ol. \n",
    "* A cada oracion del corpus, se le agrega el simbolo de inicio y fin. \n",
    "* Se crea el alfabeto $\\Sigma$ del corpus donde se almacenen unicamente los tipos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "stems = []                                              # Lista de stems por cada oracion\n",
    "cleanedCorpus = []                                      # Corpus procesado con stemming\n",
    "Sigma = []                                              # Alfabeto del corpus (tipos)               \n",
    "\n",
    "for sentence in corpus:\n",
    "    tokens = nltk.word_tokenize(sentence)               # Obtener lista tokens\n",
    "    for tk in tokens:   \n",
    "        if tk.isalpha():                                # Validar token como caracter del alfabeto                                \n",
    "            stem = stemmer.stem(tk)                     # Aplicar algotimo de stemming\n",
    "            #stem = tk\n",
    "            stems.append(stem)                          # Agregarlo a la lista de stems \n",
    "            if stem not in Sigma:                       # Agregar stem al alfabeto\n",
    "                Sigma.append(stem)\n",
    "    s = '<BOS> ' + ' '.join(stems) + ' <EOS>'           # Agregar simbolos de inicio y fin\n",
    "    cleanedCorpus.append(s)                             # Agregar oracion procesada a la lista del corpus limpio                               \n",
    "    stems.clear()\n",
    "\n",
    "# Agregar simbolos de inicio y fin al alfabeto\n",
    "Sigma.append('<BOS>')\n",
    "Sigma.append('<EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS> comenc a trabaj y me peg me maltrat con chicot <EOS>',\n",
       " '<BOS> mis patron me peg porqu no me queri apur porqu era floj <EOS>',\n",
       " '<BOS> por eso me habi peg <EOS>']"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedCorpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cab',\n",
       " 'bebecit',\n",
       " 'tabiqu',\n",
       " 'calent',\n",
       " 'pajuel',\n",
       " 'vapor',\n",
       " 'quemart',\n",
       " 'cai',\n",
       " '<BOS>',\n",
       " '<EOS>']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Obtener los bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para obtener los bigramas de una secuencia de caracteres\n",
    "def bigrams(sequence):\n",
    "    s = sequence.split()\n",
    "    return [(wi, wj) for wi, wj in zip(s[:-1], s[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los bigramas del corpus limpio\n",
    "sentence_bigrams = [bigrams(s) for s in cleanedCorpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<BOS>', 'comenc'),\n",
       " ('comenc', 'a'),\n",
       " ('a', 'trabaj'),\n",
       " ('trabaj', 'y'),\n",
       " ('y', 'me'),\n",
       " ('me', 'peg'),\n",
       " ('peg', 'me'),\n",
       " ('me', 'maltrat'),\n",
       " ('maltrat', 'con'),\n",
       " ('con', 'chicot'),\n",
       " ('chicot', '<EOS>')]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigramas de la primera oracion\n",
    "sentence_bigrams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigramas de todo el corpus\n",
    "corpus_bigrams = [bigram for sentence in sentence_bigrams for bigram in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<BOS>', 'comenc'),\n",
       " ('comenc', 'a'),\n",
       " ('a', 'trabaj'),\n",
       " ('trabaj', 'y'),\n",
       " ('y', 'me'),\n",
       " ('me', 'peg'),\n",
       " ('peg', 'me'),\n",
       " ('me', 'maltrat'),\n",
       " ('maltrat', 'con'),\n",
       " ('con', 'chicot'),\n",
       " ('chicot', '<EOS>')]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bigrams[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los vectores one hot de cada palabra en el corpus\n",
    "oneHotMatrix = np.identity(len(Sigma), np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2oneHot = {}        # Entrada: palabra del alfabeto, Salida: vector one hot\n",
    "word2number = {}        # Entrada: palabra del alfabeto, Salida: indice en la lista del alfabeto\n",
    "oneHot2word = {}        # Entrada: vector one hot (caracteres), Salida: palabra del alfabeto\n",
    "\n",
    "for i, (word, vector) in enumerate(zip(Sigma, oneHotMatrix)):\n",
    "    word2oneHot[word] = vector\n",
    "    word2number[word] = i\n",
    "    oneHot2word[np.where(vector==1)[0][0]] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2oneHot['<EOS>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Entrenar la red neuronal con los bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 300\n",
    "N = len(Sigma)\n",
    "\n",
    "U = np.random.randn(N, d)\n",
    "W = np.random.randn(d, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    h = np.dot(x, U)\n",
    "    a = np.dot(h, W)\n",
    "    return softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(bigrams, lr, epochs):\n",
    "    errorVector = []\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        for wi, wj in bigrams:\n",
    "            #print(wi, wj)\n",
    "            vectorWi = word2oneHot[wi]\n",
    "            vectorWj = word2oneHot[wj]\n",
    "            # Feedforward\n",
    "            h = np.dot(vectorWi, U)\n",
    "            a = softmax(np.dot(h, W))\n",
    "            #print(preactivation)\n",
    "            #print(output)\n",
    "            #print(preactivation)\n",
    "            error = a - vectorWj\n",
    "            #print(error)\n",
    "            # Backpropragation\n",
    "            k = np.where(vectorWi==1)[0][0]\n",
    "            W[:,k] -= lr*error[k]*h\n",
    "            delta = np.dot(W, error)\n",
    "            U[k,:] -= lr*delta\n",
    "            errorVector.append(error[k])\n",
    "    return errorVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 1/100 [00:11<18:38, 11.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 2/100 [00:22<18:20, 11.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 3/100 [00:33<18:08, 11.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 4/100 [00:41<16:15, 10.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 5/100 [00:52<16:24, 10.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 6/100 [00:59<14:57,  9.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 7/100 [01:07<13:50,  8.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 8/100 [01:14<13:03,  8.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 9/100 [01:24<13:24,  8.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 10/100 [01:35<14:15,  9.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|█         | 11/100 [01:47<15:16, 10.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▏        | 12/100 [01:58<15:35, 10.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 13%|█▎        | 13/100 [02:09<15:33, 10.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▍        | 14/100 [02:22<16:20, 11.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▌        | 15/100 [02:35<16:39, 11.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|█▌        | 16/100 [02:47<16:23, 11.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 17/100 [02:54<14:32, 10.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 18/100 [03:02<13:08,  9.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▉        | 19/100 [03:09<12:08,  8.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 20/100 [03:17<11:23,  8.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|██        | 21/100 [03:26<11:34,  8.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 22/100 [03:38<12:26,  9.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|██▎       | 23/100 [03:49<12:58, 10.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 24/100 [04:00<13:13, 10.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 25/100 [04:11<13:21, 10.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|██▌       | 26/100 [04:23<13:24, 10.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 27/100 [04:34<13:17, 10.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 28/100 [04:45<13:07, 10.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▉       | 29/100 [04:56<13:01, 11.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 30/100 [05:07<12:50, 11.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|███       | 31/100 [05:18<12:39, 11.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 32/100 [05:29<12:31, 11.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 33/100 [05:40<12:23, 11.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|███▍      | 34/100 [05:50<11:36, 10.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▌      | 35/100 [06:00<11:31, 10.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▌      | 36/100 [06:09<10:47, 10.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 37/100 [06:17<09:48,  9.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 38/100 [06:24<09:05,  8.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|███▉      | 39/100 [06:32<08:32,  8.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 40/100 [06:39<08:08,  8.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|████      | 41/100 [06:47<07:49,  7.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 42/100 [06:54<07:33,  7.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|████▎     | 43/100 [07:02<07:20,  7.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▍     | 44/100 [07:09<07:08,  7.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 45/100 [07:17<06:57,  7.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|████▌     | 46/100 [07:24<06:48,  7.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 47/100 [07:32<06:40,  7.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 48/100 [07:40<06:32,  7.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▉     | 49/100 [07:47<06:24,  7.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 50/100 [07:54<06:15,  7.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████     | 51/100 [08:02<06:07,  7.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 52/100 [08:09<05:59,  7.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 53/100 [08:17<05:52,  7.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▍    | 54/100 [08:24<05:45,  7.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 55/100 [08:32<05:37,  7.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▌    | 56/100 [08:39<05:29,  7.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████▋    | 57/100 [08:47<05:23,  7.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████▊    | 58/100 [08:55<05:16,  7.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▉    | 59/100 [09:02<05:10,  7.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 60/100 [09:10<05:01,  7.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|██████    | 61/100 [09:17<04:53,  7.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▏   | 62/100 [09:25<04:45,  7.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████▎   | 63/100 [09:32<04:36,  7.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|██████▍   | 64/100 [09:39<04:28,  7.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▌   | 65/100 [09:47<04:22,  7.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████▌   | 66/100 [09:55<04:14,  7.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 67/100 [10:02<04:06,  7.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████▊   | 68/100 [10:09<03:58,  7.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 69/100 [10:17<03:50,  7.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 70/100 [10:24<03:43,  7.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████   | 71/100 [10:32<03:36,  7.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▏  | 72/100 [10:39<03:28,  7.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████▎  | 73/100 [10:47<03:21,  7.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▍  | 74/100 [10:54<03:14,  7.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 75/100 [11:02<03:07,  7.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▌  | 76/100 [11:09<02:59,  7.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 77/100 [11:17<02:54,  7.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 78/100 [11:24<02:45,  7.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 79/100 [11:32<02:38,  7.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 80/100 [11:39<02:30,  7.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████  | 81/100 [11:47<02:22,  7.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████▏ | 82/100 [11:54<02:14,  7.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 83/100 [12:02<02:07,  7.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▍ | 84/100 [12:09<01:59,  7.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▌ | 85/100 [12:17<01:52,  7.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 86/100 [12:24<01:45,  7.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 87/100 [12:32<01:37,  7.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 88/100 [12:39<01:30,  7.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████▉ | 89/100 [12:47<01:22,  7.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 90/100 [12:54<01:14,  7.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████ | 91/100 [13:02<01:07,  7.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 92/100 [13:09<01:00,  7.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████▎| 93/100 [13:17<00:52,  7.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▍| 94/100 [13:24<00:45,  7.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 95/100 [13:32<00:37,  7.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%|█████████▌| 96/100 [13:39<00:29,  7.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 97/100 [13:47<00:22,  7.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████▊| 98/100 [13:54<00:14,  7.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████▉| 99/100 [14:02<00:07,  7.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 100/100 [14:09<00:00,  7.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "err = fit(corpus_bigrams, 0.01, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.74524154e-18 7.25955628e-13 2.95167024e-15 ... 4.60679657e-21\n",
      " 2.48340268e-21 3.38811166e-23]\n",
      "(1216,)\n",
      "1.0000000000000002\n",
      "78\n",
      "prediccion =>  el\n",
      "0.9763496271675253\n"
     ]
    }
   ],
   "source": [
    "# Prueba\n",
    "#wordVector = word2oneHot[stemmer.stem('caballo')]\n",
    "wordVector = word2oneHot['<EOS>']\n",
    "pred = predict(wordVector)\n",
    "print(pred)\n",
    "print(pred.shape)\n",
    "print(np.sum(pred))\n",
    "\n",
    "indice = np.argmax(pred)\n",
    "print(indice)\n",
    "\n",
    "print('prediccion => ', oneHot2word[indice])\n",
    "\n",
    "print(pred[indice])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Obtener las matrices $A$ y $\\Pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para cada palabra del alfabeto, predecir el vector de probabilidades\n",
    "# y agruparlos por columna para hacer la matriz A\n",
    "\n",
    "A = []\n",
    "\n",
    "for wj in Sigma[:-2]:\n",
    "    aj = predict(word2oneHot[wj])\n",
    "    A.append(list(aj))\n",
    "    \n",
    "A = np.matrix(A).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1216, 1214)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El vector de inicio se obtiene al predecir la distribucion\n",
    "# para el simbolo <BOS>\n",
    "Pi = predict(word2oneHot['<BOS>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Calcular la propabilidad de las siguientes oraciones\n",
    "\n",
    "Se calcularan usando la propiedad de Markov que establece que:\n",
    "\n",
    "$p(x_1,...,x_n)=\\prod_{i=1}^{n}p(w_{i}|w_{i-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Nos ba;amos con agua caliente\n",
    "\n",
    "$p(caliente|agua)p(agua|con)p(con|banamos)p(banamos|nos)p(nos|bos)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = 'Nos banamos con agua caliente'.split()\n",
    "s = '<BOS> pascuala ordenaba las vacas'.split()\n",
    "s[1:] = [stemmer.stem(word.lower()) for word in s[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "#j = word2number['las']\n",
    "#i = word2number['vacas']\n",
    "\n",
    "#A[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pi[word2number['pues']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(s) =  2.425589910422211e-19\n"
     ]
    }
   ],
   "source": [
    "p = 1\n",
    "\n",
    "for wi, wj in zip(s[:-1], s[1:]):\n",
    "    if wi == '<BOS>':\n",
    "        p *= Pi[word2number[wj]]\n",
    "    else:\n",
    "        i = word2number[wj]\n",
    "        j = word2number[wi]\n",
    "        p *= A[i,j]\n",
    "        \n",
    "print('p(s) = ', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "po = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'calient'"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "po.stem('caliente')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
