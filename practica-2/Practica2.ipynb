{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpusML.txt', 'r') as f:\n",
    "    corpus = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [unidecode(line.lower()) for line in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comence a trabajar y me pegaron, me maltrataron con chicote \\n',\n",
       " 'mis patrones me pegaron porque no me queria apurar, porque era flojo \\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1, 2) Limpiar corpus y agregar simbolos de inicio y fin\n",
    "\n",
    "* Se limpia el corpus mediante el algoritmo de Porter para el lenguaje español. \n",
    "* A cada oracion del corpus, se le agrega el simbolo de inicio y fin. \n",
    "* Se crea el alfabeto $\\Sigma$ del corpus donde se almacenen unicamente los tipos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "stems = []                                              # Lista de stems por cada oracion\n",
    "cleanedCorpus = []                                      # Corpus procesado con stemming\n",
    "Sigma = []                                              # Alfabeto del corpus (tipos)               \n",
    "\n",
    "for sentence in corpus:\n",
    "    tokens = nltk.word_tokenize(sentence)               # Obtener lista tokens\n",
    "    for tk in tokens:   \n",
    "        if tk.isalpha():                                # Validar token como caracter del alfabeto                                \n",
    "            stem = stemmer.stem(tk)                     # Aplicar algotimo de stemming\n",
    "            stems.append(stem)                          # Agregarlo a la lista de stems \n",
    "            if stem not in Sigma:                       # Agregar stem al alfabeto\n",
    "                Sigma.append(stem)\n",
    "    s = '<BOS> ' + ' '.join(stems) + ' <EOS>'           # Agregar simbolos de inicio y fin\n",
    "    cleanedCorpus.append(s)                             # Agregar oracion procesada a la lista del corpus limpio                               \n",
    "    stems.clear()\n",
    "\n",
    "# Agregar simbolos de inicio y fin al alfabeto\n",
    "Sigma.append('<BOS>')\n",
    "Sigma.append('<EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS> comenc a trabaj y me peg me maltrat con chicot <EOS>',\n",
       " '<BOS> mis patron me peg porqu no me queri apur porqu era floj <EOS>',\n",
       " '<BOS> por eso me habi peg <EOS>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedCorpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cab',\n",
       " 'bebecit',\n",
       " 'tabiqu',\n",
       " 'calent',\n",
       " 'pajuel',\n",
       " 'vapor',\n",
       " 'quemart',\n",
       " 'cai',\n",
       " '<BOS>',\n",
       " '<EOS>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Obtener los bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para obtener los bigramas de una secuencia de caracteres\n",
    "def bigrams(sequence):\n",
    "    s = sequence.split()\n",
    "    return [(wi, wj) for wi, wj in zip(s[:-1], s[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los bigramas del corpus limpio\n",
    "sentence_bigrams = [bigrams(s) for s in cleanedCorpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<BOS>', 'comenc'),\n",
       " ('comenc', 'a'),\n",
       " ('a', 'trabaj'),\n",
       " ('trabaj', 'y'),\n",
       " ('y', 'me'),\n",
       " ('me', 'peg'),\n",
       " ('peg', 'me'),\n",
       " ('me', 'maltrat'),\n",
       " ('maltrat', 'con'),\n",
       " ('con', 'chicot'),\n",
       " ('chicot', '<EOS>')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigramas de la primera oracion\n",
    "sentence_bigrams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigramas de todo el corpus\n",
    "corpus_bigrams = [bigram for sentence in sentence_bigrams for bigram in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<BOS>', 'comenc'),\n",
       " ('comenc', 'a'),\n",
       " ('a', 'trabaj'),\n",
       " ('trabaj', 'y'),\n",
       " ('y', 'me'),\n",
       " ('me', 'peg'),\n",
       " ('peg', 'me'),\n",
       " ('me', 'maltrat'),\n",
       " ('maltrat', 'con'),\n",
       " ('con', 'chicot'),\n",
       " ('chicot', '<EOS>')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bigrams[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los vectores one hot de cada palabra en el corpus\n",
    "oneHotMatrix = np.identity(len(Sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2oneHot = {}        # Entrada: palabra del alfabeto, Salida: vector one hot\n",
    "word2number = {}        # Entrada: palabra del alfabeto, Salida: indice en la lista del alfabeto\n",
    "oneHot2word = {}        # Entrada: vector one hot (caracteres), Salida: palabra del alfabeto\n",
    "\n",
    "for i, (word, vector) in enumerate(zip(Sigma, oneHotMatrix)):\n",
    "    word2oneHot[word] = vector\n",
    "    word2number[word] = i\n",
    "    oneHot2word[np.where(vector==1)[0][0]] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2oneHot['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1215"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2number['<EOS>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Entrenar la red neuronal con los bigramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados los bigramas del corpus $(w_i, w_j)$, la red neuronal word2vec es entrenada tomando como entrada y salida a la representacion vectorial *one hot encoded* de la palabra $w_i$ y la palabra $w_j$ respectivamente.\n",
    "\n",
    "Los vectores de entrada tienen una dimension $x\\in\\mathbb{R}^{|\\Sigma|}$ , que es el tamaño del alfabeto que obtuvimos a partir de los tipos del corpus.\n",
    "\n",
    "En cuanto a la dimension de la primera capa (embedding), sera de 300 unidades neuronales por lo que la matriz de pesos tiene una dimension $U\\in\\mathbb{R}^{300x|\\Sigma|}$.\n",
    "\n",
    "Las dimensiones de la matriz de pesos de la segunda capa son de $W\\in\\mathbb{R}^{|\\Sigma|x300}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed forward\n",
    "\n",
    "**Capa 1**\n",
    "\n",
    "La salida de la i-ésima neurona de la primera capa se calcula como: \n",
    "\n",
    "$h_i = \\sum_{j=1}^{N}U_{ij}x_j$\n",
    "\n",
    "Tambien se puede expresar de forma más general como el producto punto:\n",
    "\n",
    "$h = U \\cdot x$\n",
    "\n",
    "$h = U_{\\cdot,i}$\n",
    "\n",
    "Este vector tiene una dimension $h\\in\\mathbb{R}^{300}$, por lo que realizar el producto punto entre la matriz de pesos $U$ y el vector one hot encoded $x$ que representa a la palabra $w_i$ es equivalente a seleccionar la columna i-ésima de $U$. \n",
    "\n",
    "**Capa 2**\n",
    "\n",
    "La salida de la segunda capa se calcula como:\n",
    "\n",
    "$a = W\\cdot{h}$ \n",
    "\n",
    "$a = W_{i,\\cdot}$\n",
    "\n",
    "y tiene una dimension de $a\\in\\mathbb{R}^{|\\Sigma|}$, lo cual es equivalente a seleccionar la fila i-ésima de la matriz $W$.\n",
    "\n",
    "**Salida**\n",
    "\n",
    "La funcion de activacion a la salida de esta ultima capa es la funcion *softmax* en la cual se calcula la probabilidad de que la palabra $w_k$ donde $k=1,...,|\\Sigma|$ , aparesca despues de la palabra de entrada $w_i$, lo cual se expresa como:\n",
    "\n",
    "$p(w_k|w_i) = \\frac{e^{W\\cdot{h}}}{\\sum_{m}^{|\\Sigma|}e^{W\\cdot{h}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 300                       # Dimension del embedding\n",
    "N = len(Sigma)                # Tamaño del alfabeto\n",
    "                            \n",
    "U = np.random.randn(d, N)/np.sqrt(d+N)     # Matriz de embedding \n",
    "W = np.random.randn(N, d)/np.sqrt(d+N)     # Matriz de pesos capa 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x - np.max(x, keepdims=True)\n",
    "    return np.divide(x, np.sum(x, axis=0, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    h = np.dot(U, x)\n",
    "    a = np.dot(W, h)\n",
    "    return softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(bigrams, lr, epochs):\n",
    "    global U, W\n",
    "    lossWi = []\n",
    "    loss = []\n",
    "    for epoch in range(epochs):\n",
    "        for k, (wi, wj) in enumerate(bigrams):\n",
    "            x = word2oneHot[wi]\n",
    "            y = word2oneHot[wj]\n",
    "            # Feedforward\n",
    "            h = np.dot(U, x)\n",
    "            a = np.dot(W, h)\n",
    "            # Calcular error\n",
    "            error = softmax(a) - y\n",
    "            # -----------------------------\n",
    "            lossWi.append(np.sum(y*np.log(softmax(a)+0.001), keepdims=True))\n",
    "            if k == len(bigrams)-1:\n",
    "                loss.append(-np.sum(np.array(lossWi)))\n",
    "                lossWi.clear()\n",
    "                print('EPOCH ({}) = {}'.format(epoch+1, loss[epoch]))\n",
    "            # -----------------------------\n",
    "            # Backpropragation\n",
    "            W[np.where(y==1)[0][0],:] -= lr*error[np.where(y==1)[0][0]]*h\n",
    "            #W -= lr*np.dot(error.reshape(-1,1), h.reshape(1,-1))\n",
    "            delta = np.dot(W.T, error)\n",
    "            U[:,np.where(x==1)[0][0]] -= lr*delta\n",
    "            \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH (1) = 79509.14365173002\n",
      "EPOCH (2) = 79339.16702695756\n",
      "EPOCH (3) = 79385.8357012668\n",
      "EPOCH (4) = 79385.8410846405\n",
      "EPOCH (5) = 79385.84111827283\n",
      "EPOCH (6) = 79385.84111841819\n",
      "EPOCH (7) = 79385.84111841883\n",
      "EPOCH (8) = 79385.84111841884\n",
      "EPOCH (9) = 79385.84111841884\n",
      "EPOCH (10) = 79385.84111841884\n",
      "EPOCH (11) = 79385.84111841884\n",
      "EPOCH (12) = 79385.84111841884\n",
      "EPOCH (13) = 79385.84111841884\n",
      "EPOCH (14) = 79385.84111841884\n",
      "EPOCH (15) = 79385.84111841884\n",
      "EPOCH (16) = 79385.84111841884\n",
      "EPOCH (17) = 79385.84111841884\n",
      "EPOCH (18) = 79385.84111841884\n",
      "EPOCH (19) = 79385.84111841884\n",
      "EPOCH (20) = 79385.84111841884\n",
      "EPOCH (21) = 79385.84111841884\n",
      "EPOCH (22) = 79385.84111841884\n",
      "EPOCH (23) = 79385.84111841884\n",
      "EPOCH (24) = 79385.84111841884\n",
      "EPOCH (25) = 79385.84111841884\n",
      "EPOCH (26) = 79385.84111841884\n",
      "EPOCH (27) = 79385.84111841884\n",
      "EPOCH (28) = 79385.84111841884\n",
      "EPOCH (29) = 79385.84111841884\n",
      "EPOCH (30) = 79385.84111841884\n",
      "EPOCH (31) = 79385.84111841884\n",
      "EPOCH (32) = 79385.84111841884\n",
      "EPOCH (33) = 79385.84111841884\n",
      "EPOCH (34) = 79385.84111841884\n",
      "EPOCH (35) = 79385.84111841884\n",
      "EPOCH (36) = 79385.84111841884\n",
      "EPOCH (37) = 79515.3437539768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjkdm/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in subtract\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH (38) = nan\n",
      "EPOCH (39) = nan\n",
      "EPOCH (40) = nan\n",
      "EPOCH (41) = nan\n",
      "EPOCH (42) = nan\n",
      "EPOCH (43) = nan\n",
      "EPOCH (44) = nan\n",
      "EPOCH (45) = nan\n",
      "EPOCH (46) = nan\n",
      "EPOCH (47) = nan\n",
      "EPOCH (48) = nan\n",
      "EPOCH (49) = nan\n",
      "EPOCH (50) = nan\n",
      "EPOCH (51) = nan\n",
      "EPOCH (52) = nan\n",
      "EPOCH (53) = nan\n",
      "EPOCH (54) = nan\n",
      "EPOCH (55) = nan\n",
      "EPOCH (56) = nan\n",
      "EPOCH (57) = nan\n",
      "EPOCH (58) = nan\n",
      "EPOCH (59) = nan\n",
      "EPOCH (60) = nan\n",
      "EPOCH (61) = nan\n",
      "EPOCH (62) = nan\n",
      "EPOCH (63) = nan\n",
      "EPOCH (64) = nan\n",
      "EPOCH (65) = nan\n",
      "EPOCH (66) = nan\n",
      "EPOCH (67) = nan\n",
      "EPOCH (68) = nan\n",
      "EPOCH (69) = nan\n",
      "EPOCH (70) = nan\n",
      "EPOCH (71) = nan\n",
      "EPOCH (72) = nan\n",
      "EPOCH (73) = nan\n",
      "EPOCH (74) = nan\n",
      "EPOCH (75) = nan\n",
      "EPOCH (76) = nan\n",
      "EPOCH (77) = nan\n",
      "EPOCH (78) = nan\n",
      "EPOCH (79) = nan\n",
      "EPOCH (80) = nan\n",
      "EPOCH (81) = nan\n",
      "EPOCH (82) = nan\n",
      "EPOCH (83) = nan\n",
      "EPOCH (84) = nan\n",
      "EPOCH (85) = nan\n",
      "EPOCH (86) = nan\n",
      "EPOCH (87) = nan\n",
      "EPOCH (88) = nan\n",
      "EPOCH (89) = nan\n",
      "EPOCH (90) = nan\n",
      "EPOCH (91) = nan\n",
      "EPOCH (92) = nan\n",
      "EPOCH (93) = nan\n",
      "EPOCH (94) = nan\n",
      "EPOCH (95) = nan\n",
      "EPOCH (96) = nan\n",
      "EPOCH (97) = nan\n",
      "EPOCH (98) = nan\n",
      "EPOCH (99) = nan\n",
      "EPOCH (100) = nan\n"
     ]
    }
   ],
   "source": [
    "U = np.random.randn(d, N)/np.sqrt(d+N)     # Matriz de embedding \n",
    "W = np.random.randn(N, d)/np.sqrt(d+N)     # Matriz de pesos capa 2\n",
    "err = fit(corpus_bigrams, 0.03, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00084766 0.00064705 0.00067487 ... 0.00081128 0.00084766 0.00014396]\n",
      "(1216,)\n",
      "0.9999999999999999\n",
      "340\n",
      "prediccion =>  mientr\n",
      "0.0008649290866845483\n"
     ]
    }
   ],
   "source": [
    "# Prueba\n",
    "#wordVector = word2oneHot[stemmer.stem('caballo')]\n",
    "wordVector = word2oneHot['el']\n",
    "pred = predict(wordVector)\n",
    "print(pred)\n",
    "print(pred.shape)\n",
    "print(np.sum(pred))\n",
    "\n",
    "indice = np.argmax(pred)\n",
    "print(indice)\n",
    "\n",
    "print('prediccion => ', oneHot2word[indice])\n",
    "\n",
    "print(pred[indice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADsBJREFUeJzt23GonXd9x/H3x1xMUaFN2kRr0+xWWhjpBoqHFtkGnbVtOtAU7R/p/jBslfwx+8cUwUg3aqt/tN2kIrqNoEIQZusqYkBGia2FMUbtSduhmcZco9JrS42kFLpiS+Z3f9yn2/ldzu29uc+59+TW9wsO53l+v+95zveXA/nc53nOSVUhSdKr3jDtBiRJ5xaDQZLUMBgkSQ2DQZLUMBgkSQ2DQZLUMBgkSQ2DQZLUMBgkSY2ZaTewGhdddFHNzs5Ouw1J2lCOHj3666ratlzdhgyG2dlZhsPhtNuQpA0lyS9WUuelJElSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUmEgxJdic5nmQuyYEx85uTPNDNP5ZkdtH8ziQvJvnEJPqRJK1e72BIsgn4EnAjsAu4JcmuRWW3As9X1eXAfcA9i+bvA/61by+SpP4mccZwFTBXVSer6hXgfmDPopo9wKFu+0Hg2iQBSHITcBI4NoFeJEk9TSIYLgGeHtmf78bG1lTVGeAF4MIkbwY+Cdw5gT4kSRMwiWDImLFaYc2dwH1V9eKyb5LsTzJMMjx16tQq2pQkrcTMBI4xD1w6sr8DeGaJmvkkM8D5wGngauDmJPcCFwC/TfKbqvri4jepqoPAQYDBYLA4eCRJEzKJYHgcuCLJZcAvgb3Any+qOQzsA/4DuBl4pKoK+JNXC5J8GnhxXChIktZP72CoqjNJbgMeAjYBX62qY0nuAoZVdRj4CvC1JHMsnCns7fu+kqS1kYU/3DeWwWBQw+Fw2m1I0oaS5GhVDZar85fPkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqTGRIIhye4kx5PMJTkwZn5zkge6+ceSzHbj1yU5muQH3fN7J9GPJGn1egdDkk3Al4AbgV3ALUl2LSq7FXi+qi4H7gPu6cZ/Dby/qv4Q2Ad8rW8/kqR+JnHGcBUwV1Unq+oV4H5gz6KaPcChbvtB4Nokqaonq+qZbvwYcF6SzRPoSZK0SpMIhkuAp0f257uxsTVVdQZ4AbhwUc2HgCer6uUJ9CRJWqWZCRwjY8bqbGqSXMnC5aXrl3yTZD+wH2Dnzp1n36UkaUUmccYwD1w6sr8DeGapmiQzwPnA6W5/B/At4MNV9dOl3qSqDlbVoKoG27Ztm0DbkqRxJhEMjwNXJLksyRuBvcDhRTWHWbi5DHAz8EhVVZILgO8An6qqf59AL5KknnoHQ3fP4DbgIeBHwDeq6liSu5J8oCv7CnBhkjng48CrX2m9Dbgc+NskT3WP7X17kiStXqoW3w449w0GgxoOh9NuQ5I2lCRHq2qwXJ2/fJYkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVJjIsGQZHeS40nmkhwYM785yQPd/GNJZkfmPtWNH09ywyT6kSStXu9gSLIJ+BJwI7ALuCXJrkVltwLPV9XlwH3APd1rdwF7gSuB3cA/dMeTJE3JJM4YrgLmqupkVb0C3A/sWVSzBzjUbT8IXJsk3fj9VfVyVf0MmOuOJ0makkkEwyXA0yP7893Y2JqqOgO8AFy4wtdKktbRJIIhY8ZqhTUree3CAZL9SYZJhqdOnTrLFiVJKzWJYJgHLh3Z3wE8s1RNkhngfOD0Cl8LQFUdrKpBVQ22bds2gbYlSeNMIhgeB65IclmSN7JwM/nwoprDwL5u+2bgkaqqbnxv962ly4ArgO9PoCdJ0irN9D1AVZ1JchvwELAJ+GpVHUtyFzCsqsPAV4CvJZlj4Uxhb/faY0m+AfwXcAb4aFX9T9+eJEmrl4U/3DeWwWBQw+Fw2m1I0oaS5GhVDZar85fPkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqRGr2BIsjXJkSQnuuctS9Tt62pOJNnXjb0pyXeS/DjJsSR39+lFkjQZfc8YDgAPV9UVwMPdfiPJVuAO4GrgKuCOkQD5+6r6feBdwB8lubFnP5KknvoGwx7gULd9CLhpTM0NwJGqOl1VzwNHgN1V9VJVfQ+gql4BngB29OxHktRT32B4a1U9C9A9bx9Tcwnw9Mj+fDf2f5JcALyfhbMOSdIUzSxXkOS7wNvGTN2+wvfImLEaOf4M8HXgC1V18jX62A/sB9i5c+cK31qSdLaWDYaqet9Sc0meS3JxVT2b5GLgV2PK5oFrRvZ3AI+O7B8ETlTV55fp42BXy2AwqNeqlSStXt9LSYeBfd32PuDbY2oeAq5PsqW76Xx9N0aSzwLnA3/dsw9J0oT0DYa7geuSnACu6/ZJMkjyZYCqOg18Bni8e9xVVaeT7GDhctQu4IkkTyX5SM9+JEk9pWrjXZUZDAY1HA6n3YYkbShJjlbVYLk6f/ksSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkRq9gSLI1yZEkJ7rnLUvU7etqTiTZN2b+cJIf9ulFkjQZfc8YDgAPV9UVwMPdfiPJVuAO4GrgKuCO0QBJ8kHgxZ59SJImpG8w7AEOdduHgJvG1NwAHKmq01X1PHAE2A2Q5C3Ax4HP9uxDkjQhfYPhrVX1LED3vH1MzSXA0yP7890YwGeAzwEv9exDkjQhM8sVJPku8LYxU7ev8D0yZqySvBO4vKo+lmR2BX3sB/YD7Ny5c4VvLUk6W8sGQ1W9b6m5JM8lubiqnk1yMfCrMWXzwDUj+zuAR4H3AO9O8vOuj+1JHq2qaxijqg4CBwEGg0Et17ckaXX6Xko6DLz6LaN9wLfH1DwEXJ9kS3fT+Xrgoar6x6p6e1XNAn8M/GSpUJAkrZ++wXA3cF2SE8B13T5JBkm+DFBVp1m4l/B497irG5MknYNStfGuygwGgxoOh9NuQ5I2lCRHq2qwXJ2/fJYkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNVJV0+7hrCU5Bfxi2n2cpYuAX0+7iXXmmn83uOaN4/eqattyRRsyGDaiJMOqGky7j/Xkmn83uObXHy8lSZIaBoMkqWEwrJ+D025gClzz7wbX/DrjPQZJUsMzBklSw2CYoCRbkxxJcqJ73rJE3b6u5kSSfWPmDyf54dp33F+fNSd5U5LvJPlxkmNJ7l7f7s9Okt1JjieZS3JgzPzmJA90848lmR2Z+1Q3fjzJDevZdx+rXXOS65IcTfKD7vm96937avT5jLv5nUleTPKJ9ep5TVSVjwk9gHuBA932AeCeMTVbgZPd85Zue8vI/AeBfwZ+OO31rPWagTcBf9rVvBH4N+DGaa9piXVuAn4KvKPr9T+BXYtq/gr4p257L/BAt72rq98MXNYdZ9O017TGa34X8PZu+w+AX057PWu53pH5bwL/Anxi2uvp8/CMYbL2AIe67UPATWNqbgCOVNXpqnoeOALsBkjyFuDjwGfXoddJWfWaq+qlqvoeQFW9AjwB7FiHnlfjKmCuqk52vd7PwtpHjf5bPAhcmyTd+P1V9XJV/QyY6453rlv1mqvqyap6phs/BpyXZPO6dL16fT5jktzEwh89x9ap3zVjMEzWW6vqWYDuefuYmkuAp0f257sxgM8AnwNeWssmJ6zvmgFIcgHwfuDhNeqzr2XXMFpTVWeAF4ALV/jac1GfNY/6EPBkVb28Rn1OyqrXm+TNwCeBO9ehzzU3M+0GNpok3wXeNmbq9pUeYsxYJXkncHlVfWzxdctpW6s1jxx/Bvg68IWqOnn2Ha6L11zDMjUree25qM+aFyaTK4F7gOsn2Nda6bPeO4H7qurF7gRiQzMYzlJVvW+puSTPJbm4qp5NcjHwqzFl88A1I/s7gEeB9wDvTvJzFj6X7UkeraprmLI1XPOrDgInqurzE2h3rcwDl47s7wCeWaJmvgu784HTK3ztuajPmkmyA/gW8OGq+unat9tbn/VeDdyc5F7gAuC3SX5TVV9c+7bXwLRvcryeHsDf0d6IvXdMzVbgZyzcfN3SbW9dVDPLxrn53GvNLNxP+SbwhmmvZZl1zrBw/fgy/v/G5JWLaj5Ke2PyG932lbQ3n0+yMW4+91nzBV39h6a9jvVY76KaT7PBbz5PvYHX04OFa6sPAye651f/8xsAXx6p+0sWbkDOAX8x5jgbKRhWvWYW/iIr4EfAU93jI9Ne02us9c+An7DwzZXbu7G7gA902+ex8I2UOeD7wDtGXnt797rjnKPfvJrkmoG/Af575HN9Ctg+7fWs5Wc8cowNHwz+8lmS1PBbSZKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWr8L4G+I6VKUcyzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(err)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 73923.71882300425)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err[0][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Obtener las matrices $A$ y $\\Pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para cada palabra del alfabeto, predecir el vector de probabilidades\n",
    "# y agruparlos por columna para hacer la matriz A\n",
    "\n",
    "A = []\n",
    "\n",
    "for wj in Sigma[:-2]:\n",
    "    aj = predict(word2oneHot[wj])\n",
    "    A.append(list(aj))\n",
    "    \n",
    "A = np.matrix(A).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1216, 1214)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El vector de inicio se obtiene al predecir la distribucion\n",
    "# para el simbolo <BOS>\n",
    "Pi = predict(word2oneHot['<BOS>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Calcular la propabilidad de las siguientes oraciones\n",
    "\n",
    "Se calcularan usando la propiedad de Markov que establece que:\n",
    "\n",
    "$p(x_1,...,x_n)=\\prod_{i=1}^{n}p(w_{i}|w_{i-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Nos ba;amos con agua caliente\n",
    "\n",
    "$p(caliente|agua)p(agua|con)p(con|banamos)p(banamos|nos)p(nos|bos)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = 'Nos banamos con agua caliente'.split()\n",
    "s = '<BOS> pascuala ordenaba las vacas'.split()\n",
    "s[1:] = [stemmer.stem(word.lower()) for word in s[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "#j = word2number['las']\n",
    "#i = word2number['vacas']\n",
    "\n",
    "#A[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pi[word2number['pues']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(s) =  2.425589910422211e-19\n"
     ]
    }
   ],
   "source": [
    "p = 1\n",
    "\n",
    "for wi, wj in zip(s[:-1], s[1:]):\n",
    "    if wi == '<BOS>':\n",
    "        p *= Pi[word2number[wj]]\n",
    "    else:\n",
    "        i = word2number[wj]\n",
    "        j = word2number[wi]\n",
    "        p *= A[i,j]\n",
    "        \n",
    "print('p(s) = ', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "po = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'calient'"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "po.stem('caliente')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
