{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpusML.txt', 'r') as f:\n",
    "    corpus = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Comencé a trabajar y me pegaron, me maltrataron con chicote \\n',\n",
       " 'Mis patrones me pegaron porque no me quería apurar, porque era flojo \\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1, 2) Limpiar corpus y agregar simbolos de inicio y fin\n",
    "\n",
    "* Se limpia el corpus mediante el algoritmo de Porter para el lenguaje espa;ol. \n",
    "* A cada oracion del corpus, se le agrega el simbolo de inicio y fin. \n",
    "* Se crea el alfabeto $\\Sigma$ del corpus donde se almacenen unicamente los tipos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "stems = []                                              # Lista de stems por cada oracion\n",
    "cleanedCorpus = []                                      # Corpus procesado con stemming\n",
    "Sigma = []                                              # Alfabeto del corpus (tipos)               \n",
    "\n",
    "for sentence in corpus:\n",
    "    tokens = nltk.word_tokenize(sentence)               # Obtener lista tokens\n",
    "    for tk in tokens:   \n",
    "        if tk.isalpha():                                # Validar token como caracter del alfabeto                                \n",
    "            stem = stemmer.stem(tk)                     # Aplicar algotimo de stemming\n",
    "            stems.append(stem)                          # Agregarlo a la lista de stems \n",
    "            if stem not in Sigma:                       # Agregar stem al alfabeto\n",
    "                Sigma.append(stem)\n",
    "    s = '<BOS> ' + ' '.join(stems) + ' <EOS>'           # Agregar simbolos de inicio y fin\n",
    "    cleanedCorpus.append(s)                             # Agregar oracion procesada a la lista del corpus limpio                               \n",
    "    stems.clear()\n",
    "\n",
    "# Agregar simbolos de inicio y fin al alfabeto\n",
    "Sigma.append('<BOS>')\n",
    "Sigma.append('<EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS> comenc a trabaj y me peg me maltrat con chicot <EOS>',\n",
       " '<BOS> mis patron me peg porqu no me quer apur porqu era floj <EOS>']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedCorpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cab',\n",
       " 'bebecit',\n",
       " 'tabiqu',\n",
       " 'calent',\n",
       " 'pajuel',\n",
       " 'vapor',\n",
       " 'quemart',\n",
       " 'cai',\n",
       " '<BOS>',\n",
       " '<EOS>']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Obtener los bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para obtener los bigramas de una secuencia de caracteres\n",
    "def bigrams(sequence):\n",
    "    s = sequence.split()\n",
    "    return [(wi, wj) for wi, wj in zip(s[:-1], s[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los bigramas del corpus limpio\n",
    "corpus_bigrams = [bigrams(s) for s in cleanedCorpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<BOS>', 'comenc'),\n",
       " ('comenc', 'a'),\n",
       " ('a', 'trabaj'),\n",
       " ('trabaj', 'y'),\n",
       " ('y', 'me'),\n",
       " ('me', 'peg'),\n",
       " ('peg', 'me'),\n",
       " ('me', 'maltrat'),\n",
       " ('maltrat', 'con'),\n",
       " ('con', 'chicot'),\n",
       " ('chicot', '<EOS>')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigramas de la primera oracion\n",
    "corpus_bigrams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los vectores one hot de cada palabra en el corpus\n",
    "oneHotMatrix = np.identity(len(Sigma), np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2oneHot = {}        # Entrada: palabra del alfabeto, Salida: vector one hot\n",
    "oneHot2word = {}        # Entrada: vector one hot (caracteres), Salida: palabra del alfabeto\n",
    "\n",
    "for word, vector in zip(Sigma, oneHotMatrix):\n",
    "    word2oneHot[word] = vector\n",
    "    oneHot2word[np.array2string(vector)] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2oneHot['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<EOS>'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneHot2word[np.array2string(oneHotMatrix[-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Entrenar la red neuronal con los bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
